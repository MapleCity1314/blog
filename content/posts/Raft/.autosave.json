{
  "metadata": {
    "title": "Raft: A Simplified Implementation",
    "date": "2026-01-16",
    "description": "Raft: A Simplified Implementation",
    "tags": [
      "Distributed Systems",
      "Raft",
      "Consensus"
    ],
    "published": true
  },
  "content": "本篇文章深入解析一个用 Rust 和 Tokio 实现的简化版 Raft 共识算法，逐块分析源码的实现原理和设计思想。适合学习分布式系统、异步编程和 Rust 并发的开发者。\r\n\r\n## 项目概览\r\n\r\n本项目实现了 Raft 算法的核心机制：\r\n\r\n* Follower / Candidate / Leader 三种角色\r\n* Leader 选举和心跳\r\n* 节点间 TCP 异步通信（JSON 序列化）\r\n* 简化日志：只实现选举和心跳，不包含日志复制\r\n\r\n它使用了：\r\n\r\n* `Tokio` 异步运行时\r\n* `Arc<Mutex<Node>>` 共享节点状态\r\n* `StdRng` 保证异步安全的随机数生成\r\n* `serde` 用于 RPC 消息序列化\r\n\r\n---\r\n\r\n## 数据结构与状态\r\n\r\n```rust\n#[derive(Debug, PartialEq, Eq, Clone, Copy)]\r\nenum Role { Follower, Candidate, Leader }\r\n\r\nstruct Node {\r\n    id: String,\r\n    addr: SocketAddr,\r\n    peers: Vec<SocketAddr>,\r\n    current_term: u64,\r\n    voted_for: Option<String>,\r\n    role: Role,\r\n    votes_received: usize,\r\n}\r\n```\n\r\n### 核心要点\r\n\r\n* **Node** 保存每个节点的状态，包括网络地址、同伴列表以及 Raft 核心状态。\r\n* **current\\_term**：当前任期号\r\n* **voted\\_for**：本轮任期投票对象\r\n* **role**：节点角色\r\n* **votes\\_received**：Candidate 收到的票数\r\n\r\n共享节点状态通过 `Arc<Mutex<Node>>` 实现，保证多任务异步访问安全。\r\n\r\n---\r\n\r\n## RPC 消息定义\r\n\r\n```rust\n#[derive(Debug, Clone, Serialize, Deserialize)]\r\nenum Rpc {\r\n    RequestVote { term: u64, candidate_id: String },\r\n    RequestVoteResponse { term: u64, vote_granted: bool },\r\n    AppendEntries { term: u64, leader_id: String },\r\n    AppendEntriesResponse { term: u64, success: bool },\r\n}\r\n```\n\r\n### 原理解析\r\n\r\n* `RequestVote` / `RequestVoteResponse` 用于选举\r\n* `AppendEntries` / `AppendEntriesResponse` 用于 Leader 心跳\r\n* `serde` 自动序列化到 JSON，方便网络传输\r\n\r\n> 这里简化了日志条目，不包含 log replication。\r\n\r\n---\r\n\r\n## 节点连接与消息处理\r\n\r\n```rust\nasync fn handle_connection(mut stream: TcpStream, node: SharedNode)\r\n```\n\r\n### 关键实现\r\n\r\n* TCP 连接循环读取消息\r\n* 每条消息前 4 字节表示长度\r\n* 读取 payload 并 `serde_json` 反序列化\r\n* 调用 `process_rpc` 处理具体逻辑\r\n\r\n```rust\nasync fn process_rpc(rpc: Rpc, stream: &mut TcpStream, node: SharedNode)\r\n```\n\r\n#### 核心逻辑\r\n\r\n* `RequestVote`：判断 term、是否已投票，返回投票结果\r\n* `RequestVoteResponse`：Candidate 更新票数，如果票数达到多数则成为 Leader\r\n* `AppendEntries`：Follower 收到心跳，重置选举状态\r\n* `AppendEntriesResponse`：Leader 可用来更新 matchIndex（本 demo 未实现）\r\n\r\n> 注意：每次访问节点状态都需要 `await node.lock()`，保证状态安全。\r\n\r\n---\r\n\r\n## 选举逻辑\r\n\r\n```rust\nasync fn start_election(node: SharedNode)\r\n```\n\r\n### 实现细节\r\n\r\n1. 将自身角色改为 `Candidate`\r\n2. 当前任期加一，投自己一票\r\n3. 生成 RequestVote RPC 发送给 peers\r\n4. 使用 `StdRng` 生成随机 jitter，避免同时发起选举\r\n\r\n> 使用 `StdRng` 而非 `thread_rng` 是因为 `thread_rng` 不可 `Send`，在 `tokio::spawn` 的异步块里会导致编译错误。\r\n\r\n---\r\n\r\n## Leader 心跳\r\n\r\n```rust\nasync fn leader_heartbeat_loop(node: SharedNode)\r\n```\n\r\n* 仅 Leader 节点发送心跳\r\n* 每个 peer 都发送 `AppendEntries` 消息\r\n* 心跳间隔为 150ms\r\n* 异步 spawn，保证并发发送不阻塞主循环\r\n\r\n> 这保证了 Follower 能及时感知 Leader 存在，防止选举冲突。\r\n\r\n---\r\n\r\n## 选举超时检测\r\n\r\n```rust\nasync fn election_timeout_loop(node: SharedNode)\r\n```\n\r\n* 每个节点维护随机选举超时时间（300\\~500ms）\r\n* 超时触发新一轮选举\r\n* Leader 存在时会重置计时器\r\n* 使用 `StdRng` 生成异步安全随机数\r\n\r\n> 核心思想：**Raft 利用随机化选举超时避免 split vote**。\r\n\r\n---\r\n\r\n## 主函数与节点启动\r\n\r\n```rust\n#[tokio::main]\r\nasync fn main() -> anyhow::Result<()> { ... }\r\n```\n\r\n* 解析命令行参数（监听地址 + peer 列表）\r\n* 创建节点状态 `Arc<Mutex<Node>>`\r\n* 启动三个核心异步任务：\r\n\r\n  1. `listener_task`：TCP 接收连接\r\n  2. `leader_heartbeat_loop`：Leader 心跳\r\n  3. `election_timeout_loop`：选举超时\r\n* 主循环每 2 秒打印节点状态\r\n\r\n> 使用 `tokio::spawn` 并发运行异步任务，实现高性能非阻塞网络节点。\r\n\r\n---\r\n\r\n## 总结与优化思路\r\n\r\n### 优点\r\n\r\n* 纯 Rust + Tokio 实现，线程安全且高性能\r\n* 简化版 Raft，核心选举逻辑完整\r\n* 异步 TCP RPC，适合分布式模拟实验\r\n\r\n### 局限\r\n\r\n* 未实现日志复制、持久化、快照\r\n* 没有处理网络分区和节点崩溃恢复\r\n* 日志、RPC 可靠性未处理\r\n\r\n### 可扩展方向\r\n\r\n1. 加入日志条目 replication，实现完整 Raft\r\n2. 持久化节点状态到磁盘\r\n3. 支持网络分区与恢复\r\n4. 优化 RPC，支持异步批量发送\r\n\r\n## 完整代码\r\ncargo.toml\r\n```toml\n[package]\r\nname = \"raft\"\r\nversion = \"0.1.0\"\r\nedition = \"2024\"\r\n\r\n[dependencies]\r\ntokio = { version = \"1.34\", features = [\"full\"] }\r\nserde = { version = \"1.0\", features = [\"derive\"] }\r\nserde_json = \"1.0\"\r\nrand = \"0.8\"\r\nuuid = { version = \"1.4\", features = [\"v4\"] }\r\nlog = \"0.4\"\r\nenv_logger = \"0.10\"\r\nanyhow = \"1.0.100\"\r\n```\n\r\nmain.rs\r\n```rust\nuse serde::{Deserialize, Serialize};\r\nuse std::{env, net::SocketAddr, sync::Arc, time::Duration};\r\nuse tokio::{\r\n    io::{AsyncReadExt, AsyncWriteExt},\r\n    net::{TcpListener, TcpStream},\r\n    sync::Mutex,\r\n    time,\r\n};\r\nuse rand::{rngs::StdRng, Rng, SeedableRng};\r\nuse uuid::Uuid;\r\nuse log::{info, warn, debug};\r\n\r\n#[derive(Debug, Clone, Serialize, Deserialize)]\r\nenum Rpc {\r\n    RequestVote {\r\n        term: u64,\r\n        candidate_id: String,\r\n    },\r\n    RequestVoteResponse {\r\n        term: u64,\r\n        vote_granted: bool,\r\n    },\r\n    AppendEntries {\r\n        term: u64,\r\n        leader_id: String,\r\n        // simplified: no log entries in this demo\r\n    },\r\n    AppendEntriesResponse {\r\n        term: u64,\r\n        success: bool,\r\n    },\r\n}\r\n\r\n#[derive(Debug, PartialEq, Eq, Clone, Copy)]\r\nenum Role {\r\n    Follower,\r\n    Candidate,\r\n    Leader,\r\n}\r\n\r\nstruct Node {\r\n    id: String,\r\n    addr: SocketAddr,\r\n    peers: Vec<SocketAddr>,\r\n\r\n    // volatile state\r\n    current_term: u64,\r\n    voted_for: Option<String>,\r\n    role: Role,\r\n    votes_received: usize,\r\n}\r\n\r\ntype SharedNode = Arc<Mutex<Node>>;\r\n\r\nimpl Node {\r\n    fn new(id: String, addr: SocketAddr, peers: Vec<SocketAddr>) -> Self {\r\n        Self {\r\n            id,\r\n            addr,\r\n            peers,\r\n            current_term: 0,\r\n            voted_for: None,\r\n            role: Role::Follower,\r\n            votes_received: 0,\r\n        }\r\n    }\r\n}\r\n\r\nasync fn handle_connection(mut stream: TcpStream, node: SharedNode) {\r\n    let mut buf = Vec::new();\r\n    loop {\r\n        let mut len_buf = [0u8; 4];\r\n        if let Err(e) = stream.read_exact(&mut len_buf).await {\r\n            debug!(\"read_exact len failed: {}\", e);\r\n            break;\r\n        }\r\n        let len = u32::from_be_bytes(len_buf) as usize;\r\n        buf.resize(len, 0);\r\n        if let Err(e) = stream.read_exact(&mut buf).await {\r\n            debug!(\"read_exact payload failed: {}\", e);\r\n            break;\r\n        }\r\n        let rpc: Rpc = match serde_json::from_slice(&buf) {\r\n            Ok(r) => r,\r\n            Err(e) => {\r\n                warn!(\"failed deserialize rpc: {}\", e);\r\n                continue;\r\n            }\r\n        };\r\n        process_rpc(rpc, &mut stream, node.clone()).await;\r\n    }\r\n}\r\n\r\nasync fn process_rpc(rpc: Rpc, stream: &mut TcpStream, node: SharedNode) {\r\n    match rpc {\r\n        Rpc::RequestVote { term, candidate_id } => {\r\n            let mut n = node.lock().await;\r\n            if term > n.current_term {\r\n                n.current_term = term;\r\n                n.voted_for = None;\r\n                n.role = Role::Follower;\r\n            }\r\n            let vote_granted = if (n.voted_for.is_none() || n.voted_for.as_deref() == Some(&candidate_id))\r\n                && term >= n.current_term\r\n            {\r\n                n.voted_for = Some(candidate_id.clone());\r\n                true\r\n            } else {\r\n                false\r\n            };\r\n            debug!(\"{}: RequestVote from {} term {} -> grant: {}\", n.id, candidate_id, term, vote_granted);\r\n            let resp = Rpc::RequestVoteResponse {\r\n                term: n.current_term,\r\n                vote_granted,\r\n            };\r\n            let _ = send_rpc_direct(stream, &resp).await;\r\n        }\r\n        Rpc::RequestVoteResponse { term, vote_granted } => {\r\n            let mut n = node.lock().await;\r\n            if term > n.current_term {\r\n                n.current_term = term;\r\n                n.role = Role::Follower;\r\n                n.voted_for = None;\r\n                n.votes_received = 0;\r\n            } else {\r\n                if n.role == Role::Candidate && vote_granted {\r\n                    n.votes_received += 1;\r\n                    let quorum = (n.peers.len() + 1) / 2 + 1;\r\n                    info!(\"{}: got vote (total {})\", n.id, n.votes_received);\r\n                    if n.votes_received >= quorum {\r\n                        info!(\"{}: becomes Leader in term {}\", n.id, n.current_term);\r\n                        n.role = Role::Leader;\r\n                        // when becoming leader, reset leader-specific state if any\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        Rpc::AppendEntries { term, leader_id } => {\r\n            let mut n = node.lock().await;\r\n            if term >= n.current_term {\r\n                n.current_term = term;\r\n                n.role = Role::Follower;\r\n                n.voted_for = Some(leader_id.clone());\r\n                // reset election timer would happen in the main loop\r\n                debug!(\"{}: received heartbeat from leader {} term {}\", n.id, leader_id, term);\r\n                let resp = Rpc::AppendEntriesResponse {\r\n                    term: n.current_term,\r\n                    success: true,\r\n                };\r\n                let _ = send_rpc_direct(stream, &resp).await;\r\n            } else {\r\n                let resp = Rpc::AppendEntriesResponse {\r\n                    term: n.current_term,\r\n                    success: false,\r\n                };\r\n                let _ = send_rpc_direct(stream, &resp).await;\r\n            }\r\n        }\r\n        Rpc::AppendEntriesResponse { term, success } => {\r\n            let mut n = node.lock().await;\r\n            if term > n.current_term {\r\n                n.current_term = term;\r\n                n.role = Role::Follower;\r\n                n.voted_for = None;\r\n                n.votes_received = 0;\r\n            } else {\r\n                // leader could use success to update matchIndex; omitted in this simplified demo\r\n                debug!(\"{}: AppendEntriesResponse success={}\", n.id, success);\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nasync fn send_rpc(addr: &SocketAddr, rpc: &Rpc) {\r\n    match TcpStream::connect(addr).await {\r\n        Ok(mut s) => {\r\n            if let Err(e) = send_rpc_direct(&mut s, rpc).await {\r\n                debug!(\"send_rpc write failed: {}\", e);\r\n            }\r\n        }\r\n        Err(e) => {\r\n            debug!(\"connect failed to {}: {}\", addr, e);\r\n        }\r\n    }\r\n}\r\n\r\nasync fn send_rpc_direct(stream: &mut TcpStream, rpc: &Rpc) -> Result<(), std::io::Error> {\r\n    let payload = serde_json::to_vec(rpc).unwrap();\r\n    let len = (payload.len() as u32).to_be_bytes();\r\n    stream.write_all(&len).await?;\r\n    stream.write_all(&payload).await?;\r\n    Ok(())\r\n}\r\n\r\nasync fn listener_task(node: SharedNode) -> anyhow::Result<()> {\r\n    let addr;\r\n    {\r\n        let n = node.lock().await;\r\n        addr = n.addr;\r\n    }\r\n    let listener = TcpListener::bind(addr).await?;\r\n    info!(\"{}: listening on {}\", node.lock().await.id, addr);\r\n    loop {\r\n        let (socket, _peer) = listener.accept().await?;\r\n        let node_cl = node.clone();\r\n        tokio::spawn(async move {\r\n            handle_connection(socket, node_cl).await;\r\n        });\r\n    }\r\n}\r\n\r\nasync fn start_election(node: SharedNode) {\r\n    let mut rng = StdRng::from_entropy(); // <- StdRng 是 Send\r\n\r\n    let mut n = node.lock().await;\r\n    n.role = Role::Candidate;\r\n    n.current_term += 1;\r\n    n.voted_for = Some(n.id.clone());\r\n    n.votes_received = 1; // vote for self\r\n    let term = n.current_term;\r\n    let candidate_id = n.id.clone();\r\n    let peers = n.peers.clone();\r\n    info!(\"{}: starting election for term {}\", n.id, term);\r\n    drop(n);\r\n\r\n    for p in peers {\r\n        let rpc = Rpc::RequestVote {\r\n            term,\r\n            candidate_id: candidate_id.clone(),\r\n        };\r\n        let node_cl = node.clone();\r\n        tokio::spawn(async move {\r\n            send_rpc(&p, &rpc).await;\r\n            // responses handled by listener; keep ownership to satisfy move\r\n            let _ = node_cl;\r\n        });\r\n\r\n        // small jitter to avoid bursts (现在用 StdRng)\r\n        let ms: u64 = rng.gen_range(5..25);\r\n        time::sleep(Duration::from_millis(ms)).await;\r\n    }\r\n}\r\n\r\nasync fn leader_heartbeat_loop(node: SharedNode) {\r\n    loop {\r\n        {\r\n            let n = node.lock().await;\r\n            if n.role != Role::Leader {\r\n                // only leader sends heartbeats here\r\n                drop(n);\r\n                time::sleep(Duration::from_millis(50)).await;\r\n                continue;\r\n            }\r\n        }\r\n        let _n_clone = node.clone();\r\n        let (peers, term, leader_id) = {\r\n            let n = node.lock().await;\r\n            (n.peers.clone(), n.current_term, n.id.clone())\r\n        };\r\n        for p in peers {\r\n            let rpc = Rpc::AppendEntries {\r\n                term,\r\n                leader_id: leader_id.clone(),\r\n            };\r\n            let paddr = p;\r\n            tokio::spawn(async move {\r\n                send_rpc(&paddr, &rpc).await;\r\n            });\r\n        }\r\n        time::sleep(Duration::from_millis(150)).await; // heartbeat interval\r\n    }\r\n}\r\n\r\nasync fn election_timeout_loop(node: SharedNode) {\r\n    // StdRng 是 Send + Sync，可以安全地跨线程\r\n    let mut rng = StdRng::from_entropy();\r\n\r\n    loop {\r\n        // 生成随机超时时间\r\n        let timeout: u64 = rng.gen_range(300..500);\r\n\r\n        let mut elapsed = 0u64;\r\n        let step = 50u64;\r\n        let mut reset = false;\r\n\r\n        while elapsed < timeout {\r\n            time::sleep(Duration::from_millis(step)).await;\r\n            elapsed += step;\r\n\r\n            let n = node.lock().await;\r\n            if n.role == Role::Leader {\r\n                reset = true;\r\n                break;\r\n            }\r\n        }\r\n\r\n        if reset {\r\n            continue;\r\n        }\r\n\r\n        {\r\n            let n = node.lock().await;\r\n            if n.role != Role::Leader {\r\n                drop(n);\r\n                start_election(node.clone()).await;\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n#[tokio::main]\r\nasync fn main() -> anyhow::Result<()> {\r\n    env_logger::init();\r\n\r\n    let args: Vec<String> = env::args().collect();\r\n    // expect: <bin> <listen_addr> <peer1> <peer2> ...\r\n    if args.len() < 2 {\r\n        println!(\"Usage: {} <listen_addr> [peer_addr...]\", args[0]);\r\n        println!(\"Example: cargo run -- 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002\");\r\n        return Ok(());\r\n    }\r\n    let listen_addr: SocketAddr = args[1].parse()?;\r\n    let mut peers = Vec::new();\r\n    for a in args.iter().skip(2) {\r\n        let sa: SocketAddr = a.parse()?;\r\n        peers.push(sa);\r\n    }\r\n\r\n    let id = Uuid::new_v4().to_string();\r\n    let node = Arc::new(Mutex::new(Node::new(id.clone(), listen_addr, peers)));\r\n    info!(\"Node id: {}\", id);\r\n\r\n    // start listener\r\n    let node_cl = node.clone();\r\n    tokio::spawn(async move {\r\n        if let Err(e) = listener_task(node_cl).await {\r\n            panic!(\"listener failed: {}\", e);\r\n        }\r\n    });\r\n\r\n    // heartbeat loop (leaders only actually send)\r\n    let node_cl = node.clone();\r\n    tokio::spawn(async move {\r\n        leader_heartbeat_loop(node_cl).await;\r\n    });\r\n\r\n    // election timeout loop\r\n    let node_cl = node.clone();\r\n    tokio::spawn(async move {\r\n        election_timeout_loop(node_cl).await;\r\n    });\r\n\r\n    // keep main alive and periodically print status\r\n    loop {\r\n        {\r\n            let n = node.lock().await;\r\n            info!(\"Status: id={} role={:?} term={} voted_for={:?}\",\r\n                n.id, n.role, n.current_term, n.voted_for);\r\n        }\r\n        time::sleep(Duration::from_secs(2)).await;\r\n    }\r\n}\r\n\r\n```\n\r\n",
  "savedAt": "2026-02-10T12:56:07.157Z"
}